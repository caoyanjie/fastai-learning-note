{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB 电影影评数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'texts.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextDataBunch.from_csv(path, 'texts.csv')\n",
    "data_lm.save()   # 预处理已经做过了，下次加载就不用重新做了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = TextDataBunch.load(path)  # API 已弃用\n",
    "data = load_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization  分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = TextClasDataBunch.load(path) # 因为下载的文件没有tmp/.pkl\n",
    "data = load_data(path)\n",
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalization  数值化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.vocab.itos[:10]  # 那个汇集了所有不重复标记的列表，可以成为词汇表，简写成vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_ds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.train_ds[0][0][:10]\n",
    "data.train_ds[0][0].data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the data block API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = (TextSplitData.from_csv(path, 'texts.csv', input_cols=1, label_cols=0, valid_col=2)\n",
    "data = (TextList.from_csv(path, 'texts.csv', col='text')\n",
    "       .datasets(TextDataset)\n",
    "       .tokenize()\n",
    "       .numericalize()\n",
    "       .databunch(TextDataBunch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Downloading https://s3.amazonaws.com/fast-ai-nlp/imdb\n",
    "0.00% [0/144440600 00:00<00:00]\n",
    " Download of https://s3.amazonaws.com/fast-ai-nlp/imdb.tgz has failed after 5 retries\n",
    " Fix the download manually:\n",
    "$ mkdir -p C:\\Users\\xiaop\\.fastai\\data\n",
    "$ cd C:\\Users\\xiaop\\.fastai\\data\n",
    "$ wget -c https://s3.amazonaws.com/fast-ai-nlp/imdb.tgz\n",
    "$ tar -zxvf imdb.tgz\n",
    "And re-run your code once the download is successful\n",
    "'''\n",
    "path = untar_data(URLs.IMDB)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langurage model  创建语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_lm = (TextFileList.from_folder(path)  # API 已弃用\n",
    "#          .label_const(0)\n",
    "#          .split_by_folder(valid='test')\n",
    "#          .datasets()\n",
    "#          .tokenize()\n",
    "#          .numericalize()\n",
    "#          .databunch(TextLMDataBunch))\n",
    "data_lm = (TextList.from_folder(path)\n",
    "           .filter_by_folder(include=['train', 'test'])\n",
    "           .random_split_by_pct(0.1)\n",
    "           .label_for_lm()\n",
    "           .databunch())\n",
    "data_lm.save('tmp_lm')\n",
    "data_lm = TextLMDataBunch.load(path, 'tmp_lm')\n",
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, pretrained_model=URLs.WT103, drop_mult=0.3) # RNN 神经网络\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 1e-2, moms=(0.8, 0.7))\n",
    "learn.save('fit_head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fit_head')\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 1e-3, moms=(0.8, 0.7))\n",
    "learn.save('fine_tuned')\n",
    "learn.save_encoder('fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fine_tuned')\n",
    "learn.predict('I liked this movie because ', 100, temperature=1.1, min_p=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_clas = (TextFileList.from_folder(path)  # API 已弃用\n",
    "#            .label_from_folder(classes=['neg', 'pos'])\n",
    "#            .split_by_folder(valid='test')\n",
    "#            .datasets()\n",
    "#            .tokenize()\n",
    "#            .numericalize(vocab=data_lm.vocab)\n",
    "#            .databunch(TextClasDataBunch, bs=50))  # bs根据GPU内存调整\n",
    "data_clas = (TextFileList.from_folder(path, vocab=data_lm.vocab)\n",
    "             .split_by_folder(valid='test')\n",
    "             .label_from_folder(class=['neg', 'pos'])\n",
    "             .databunch(bs=50))\n",
    "data_clas.save('tmp_clas')\n",
    "data_clas = TextClasDataBunch.load(path, 'tmp_clas', bs=50)\n",
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clas, drop_mult=0.5)\n",
    "learn.load_encoder('fine_tuned_enc')\n",
    "learn.freeze()\n",
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 2e-2, moms=(0,8, 0.7))\n",
    "learn.save('first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('first')\n",
    "larn.freeze_to(-2)  # 解冻最后两层\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4), 1e-2), moms=(0.8,0.7))\n",
    "learn.save('second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('second')\n",
    "learn.freeze_to(-3)  # 解冻最后三层\n",
    "laarn.fit_one_cycle(1, slice(5e-3/(2.6**4), 5e-3), moms=(0.8,0.7))\n",
    "learn.save('third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('third')\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4), 1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tablular example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.tabular import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.ADULT_SAMPLE)\n",
    "df = pd.read_csv(path/'adult.csv')\n",
    "train_df, valid_df = df[:-2000].copy(), df[-2000:].copy()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.tabular import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.ADULT_SAMPLE)\n",
    "df = pd.read_csv(path/'adult.csv')\n",
    "dep_var = '>=50k'\n",
    "cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
    "cont_names = ['age', 'fnlwgt', 'education-num']\n",
    "procs = [FillMissing, Categorify, Normalize]\n",
    "test = TabularList.from_df(df.ilo[800:1000].copy(), path=path, cat_names=cat_names, cont_names=cont_names)\n",
    "data = (TabularList.from_df(df, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "       .split_by_idx(list(range(800, 1000)))\n",
    "       .label_from_df(cols=dep_var)\n",
    "       .add_test(test, label=0)\n",
    "       .databunch())\n",
    "data.show_batch(rows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_tabular_learner(data, layers=[200, 100], metrics=accuracy)\n",
    "learn.fit(1, 1e-2)\n",
    "df = pd.read_csv(path/'adult.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering example  协同过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.ML_SAMPLE)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(path/'ratings.csv')\n",
    "series2cat(ratings, 'userId', 'movieId')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_collab_learner(ratings, n_factors=50, min_score=0., max_score=5.)\n",
    "learn.fit_one_cycle(4, 5e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
